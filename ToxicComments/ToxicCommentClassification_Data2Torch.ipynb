{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAD - Unidad 3 - Toxic Comments Clasification\n",
    "###  JSON to Torch\n",
    "#### Integrantes:\n",
    "- C. Cárdenas\n",
    "- A. Morales\n",
    "- M.J. Núñez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descripción de la data\n",
    "\n",
    "Se le proporciona una gran cantidad de comentarios de Wikipedia que han sido etiquetados por evaluadores humanos por su comportamiento tóxico. Los tipos de toxicidad son:\n",
    "\n",
    "\n",
    "    toxic (tóxico)\n",
    "    severe_toxic (muy tóxico)\n",
    "    obscene (obsceno)\n",
    "    threat (amenasa)\n",
    "    insult (insulto)\n",
    "    identity_hate (odio)\n",
    "\n",
    "- `train.csv`: el conjunto de entrenamiento, contiene comentarios con sus etiquetas binarias\n",
    "- `test.csv`: el conjunto de prueba, debe predecir las probabilidades de toxicidad para estos comentarios. Para disuadir el etiquetado manual, el conjunto de prueba contiene algunos comentarios que no están incluidos en la puntuación.\n",
    "- `sample_submission.csv`: un archivo de envío de muestra en el formato correcto\n",
    "- `test_labels.csv`: etiquetas para los datos de prueba; el valor de $-1$ indica que no se usó para calificar; (Nota: ¡archivo agregado después del cierre de la competencia!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import spacy\n",
    "import random\n",
    "#import json\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "#from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==1.5.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/cf/007b6de316c9f3d4cb315a60c308342cc299e464167f5ebc369e93b5e23a/torch-1.5.1-cp37-cp37m-manylinux1_x86_64.whl (753.2MB)\n",
      "\u001b[K     |████████████████████████████████| 753.2MB 87kB/s s eta 0:00:01    |█▋                              | 37.1MB 21.4MB/s eta 0:00:34     |███████▍                        | 173.4MB 21.2MB/s eta 0:00:28     |████████▉                       | 208.8MB 15.6MB/s eta 0:00:35     |█████████▌                      | 223.4MB 15.6MB/s eta 0:00:34     |██████████                      | 235.2MB 20.2MB/s eta 0:00:26     |███████████▌                    | 271.5MB 25.4MB/s eta 0:00:19     |████████████                    | 283.6MB 21.4MB/s eta 0:00:22     |████████████▍                   | 292.6MB 21.4MB/s eta 0:00:22     |█████████████▎                  | 312.1MB 23.4MB/s eta 0:00:19     |█████████████▋                  | 321.1MB 23.4MB/s eta 0:00:19     |████████████████████▋           | 485.8MB 22.0MB/s eta 0:00:13     |█████████████████████▌          | 507.3MB 22.0MB/s eta 0:00:12     |██████████████████████          | 516.4MB 18.4MB/s eta 0:00:13     |███████████████████████         | 539.6MB 18.6MB/s eta 0:00:12     |███████████████████████         | 542.3MB 18.6MB/s eta 0:00:12     |████████████████████████▌       | 577.0MB 20.4MB/s eta 0:00:09     |████████████████████████▊       | 581.4MB 20.4MB/s eta 0:00:09     |█████████████████████████▉      | 609.1MB 24.3MB/s eta 0:00:06     |██████████████████████████▏     | 615.9MB 27.3MB/s eta 0:00:06     |██████████████████████████▍     | 621.6MB 27.3MB/s eta 0:00:05     |███████████████████████████▌    | 648.0MB 15.8MB/s eta 0:00:07     |████████████████████████████▎   | 665.6MB 17.0MB/s eta 0:00:06     |████████████████████████████▌   | 671.0MB 17.0MB/s eta 0:00:05     |███████████████████████████████ | 730.9MB 18.8MB/s eta 0:00:02     |███████████████████████████████▊| 747.8MB 35.0MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: future in /home/amorales/anaconda3/envs/IA/lib/python3.7/site-packages (from torch==1.5.1) (0.17.1)\n",
      "Requirement already satisfied: numpy in /home/amorales/anaconda3/envs/IA/lib/python3.7/site-packages (from torch==1.5.1) (1.19.1)\n",
      "Installing collected packages: torch\n",
      "  Found existing installation: torch 1.2.0\n",
      "    Uninstalling torch-1.2.0:\n",
      "      Successfully uninstalled torch-1.2.0\n",
      "Successfully installed torch-1.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.1 2.3.0 0.6.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__,spacy.__version__,torchtext.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se definen los _Fields_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize='spacy', batch_first = True)\n",
    "TOXIC = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [(None, None),(None, None),('comment_text', TEXT),(None, None),(None, None),(None, None),(None, None),(None, None),(None, None),(None, None),(None, None),('toxicity', TOXIC)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se leen los CSV para tokenizarlos con Torchtext.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "train_data, valid_data, test_data = data.TabularDataset.splits(\n",
    "                                        path = '../challenge_nlp/data',\n",
    "                                        train = 'train_data.csv',\n",
    "                                        validation= 'valid_data.csv',\n",
    "                                        test = 'test_data.csv',\n",
    "                                        format = 'csv',\n",
    "                                        fields = fields,\n",
    "                                        skip_header = True\n",
    ")\n",
    "\n",
    "#train_data, valid_data = train_data.split(random_state = random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "883\n",
      "69\n",
      "69\n",
      "54\n",
      "73\n",
      "582\n",
      "21\n",
      "143\n",
      "64\n",
      "45\n",
      "67\n",
      "170\n",
      "6\n",
      "29\n",
      "34\n",
      "27\n",
      "172\n",
      "37\n",
      "120\n",
      "66\n",
      "32\n",
      "25\n",
      "113\n",
      "120\n",
      "36\n",
      "80\n",
      "11\n",
      "108\n",
      "40\n",
      "190\n",
      "47\n",
      "24\n",
      "42\n",
      "25\n",
      "33\n",
      "377\n",
      "52\n",
      "20\n",
      "6\n",
      "105\n",
      "75\n",
      "12\n",
      "23\n",
      "62\n",
      "10\n",
      "5\n",
      "74\n",
      "67\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    print(vars(valid_data[i])['comment_text'].__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.data.example.Example at 0x7fde50d35790>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(test_data[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'comment_text': ['stuff', 'keilor', 'downs', 'u', ' ', 'suck'], 'toxicity': '1'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(test_data[12]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25000\n",
    "\n",
    "TEXT.build_vocab(train_data, \n",
    "                 max_size = MAX_VOCAB_SIZE, \n",
    "                 vectors = \"glove.6B.100d\", \n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "\n",
    "TOXIC.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE, \n",
    "    device = device,\n",
    "    #sort_key=None,\n",
    "    sort_key=lambda x:len(x.toxicity),\n",
    "    sort_within_batch=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "\n",
      "[torchtext.data.batch.Batch of size 32]\n",
      "\t[.comment_text]:[torch.cuda.LongTensor of size 32x329 (GPU 0)]\n",
      "\t[.toxicity]:[torch.cuda.FloatTensor of size 32 (GPU 0)]\n"
     ]
    }
   ],
   "source": [
    "print('Train:')\n",
    "for batch in train_iterator:\n",
    "    print(batch)\n",
    "    break\n",
    "    #largo = batch.comment_text[0].__len__()\n",
    "    #if largo < 10:\n",
    "    #    print(largo)\n",
    "    #print(batch.comment_text[j])\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN1d(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
    "                 dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        self.convs = nn.ModuleList([\n",
    "                                    nn.Conv1d(in_channels = embedding_dim, \n",
    "                                              out_channels = n_filters, \n",
    "                                              kernel_size = fs)\n",
    "                                    for fs in filter_sizes\n",
    "                                    ])\n",
    "        \n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        #text = [batch size, sent len]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "                \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        embedded = embedded.permute(0, 2, 1)\n",
    "        \n",
    "        #embedded = [batch size, emb dim, sent len]\n",
    "        \n",
    "        conved = [F.relu(conv(embedded)) for conv in self.convs]\n",
    "            \n",
    "        #conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
    "        \n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        \n",
    "        #pooled_n = [batch size, n_filters]\n",
    "        \n",
    "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
    "        \n",
    "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "            \n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [3]\n",
    "OUTPUT_DIM = 1\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = CNN1d(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "a = 0.499\n",
    "b = 0.5001\n",
    "\n",
    "print(torch.round(torch.tensor(a)))\n",
    "print(torch.round(torch.tensor(b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(batch.comment_text).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.toxicity)\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch.toxicity)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "           \n",
    "            predictions = model(batch.comment_text).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.toxicity)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.toxicity)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[torchtext.data.batch.Batch of size 32]\n",
      "\t[.comment_text]:[torch.cuda.LongTensor of size 32x989 (GPU 0)]\n",
      "\t[.toxicity]:[torch.cuda.FloatTensor of size 32 (GPU 0)]\n"
     ]
    }
   ],
   "source": [
    "#print(evaluate(model, valid_iterator, criterion))\n",
    "for batch in valid_iterator:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 40s\n",
      "\tTrain Loss: 0.138 | Train Acc: 95.15%\n",
      "\t Val. Loss: 0.113 |  Val. Acc: 95.88%\n",
      "Epoch: 02 | Epoch Time: 0m 40s\n",
      "\tTrain Loss: 0.101 | Train Acc: 96.31%\n",
      "\t Val. Loss: 0.104 |  Val. Acc: 96.21%\n",
      "Epoch: 03 | Epoch Time: 0m 39s\n",
      "\tTrain Loss: 0.086 | Train Acc: 96.84%\n",
      "\t Val. Loss: 0.109 |  Val. Acc: 96.18%\n",
      "Epoch: 04 | Epoch Time: 0m 40s\n",
      "\tTrain Loss: 0.073 | Train Acc: 97.28%\n",
      "\t Val. Loss: 0.128 |  Val. Acc: 95.99%\n",
      "Epoch: 05 | Epoch Time: 0m 40s\n",
      "\tTrain Loss: 0.061 | Train Acc: 97.75%\n",
      "\t Val. Loss: 0.134 |  Val. Acc: 95.96%\n",
      "Epoch: 06 | Epoch Time: 0m 40s\n",
      "\tTrain Loss: 0.052 | Train Acc: 98.15%\n",
      "\t Val. Loss: 0.154 |  Val. Acc: 95.73%\n",
      "Epoch: 07 | Epoch Time: 0m 40s\n",
      "\tTrain Loss: 0.045 | Train Acc: 98.45%\n",
      "\t Val. Loss: 0.167 |  Val. Acc: 95.73%\n",
      "Epoch: 08 | Epoch Time: 0m 38s\n",
      "\tTrain Loss: 0.038 | Train Acc: 98.72%\n",
      "\t Val. Loss: 0.198 |  Val. Acc: 95.53%\n",
      "Epoch: 09 | Epoch Time: 0m 38s\n",
      "\tTrain Loss: 0.033 | Train Acc: 98.88%\n",
      "\t Val. Loss: 0.230 |  Val. Acc: 95.51%\n",
      "Epoch: 10 | Epoch Time: 0m 40s\n",
      "\tTrain Loss: 0.030 | Train Acc: 99.03%\n",
      "\t Val. Loss: 0.255 |  Val. Acc: 95.50%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        nombre = './Models/challenge-model-CNN'+'_ep'+str(epoch+1)+'.pt'\n",
    "        torch.save({'epoca': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'Valid_loss': best_valid_loss}, nombre)\n",
    "        #torch.save(model.state_dict(), nombre)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargar mejor modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mejorcito = CNN1d(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "mejorcito.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "mejorcito.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "mejorcito.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nombre = './Models/challenge-model-CNN'+'_ep'+str(2)+'.pt'\n",
    "mejorcito.load_state_dict(torch.load(nombre)['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score,confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mejorcito = mejorcito.cuda()\n",
    "prediction_test = []\n",
    "labels_test=[]\n",
    "for batch in test_iterator:\n",
    "    labels_test.append(batch.toxicity.cpu().detach().numpy())\n",
    "    predictions = mejorcito(batch.comment_text.cpu()).squeeze(1)\n",
    "    rounded_preds = torch.round(torch.sigmoid(predictions))\n",
    "    prediction_test.append(rounded_preds.detach().numpy())\n",
    "    \n",
    "\n",
    "y_true = np.concatenate(labels_test)\n",
    "y_pred = np.concatenate(prediction_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(y_pred,y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[28123,   544],\n",
       "       [  817,  2429]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.98      0.98     28667\n",
      "         1.0       0.82      0.75      0.78      3246\n",
      "\n",
      "    accuracy                           0.96     31913\n",
      "   macro avg       0.89      0.86      0.88     31913\n",
      "weighted avg       0.96      0.96      0.96     31913\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_true, y_pred)\n",
    "display(cm)\n",
    "\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementamos un CSV con comentarios y etiquetas manuales par aextender la generalización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "campos =[('comment_text',TEXT),('toxicity',TOXIC)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_data = data.TabularDataset(\n",
    "                                        path = '../challenge_nlp/data/manual_data.csv',\n",
    "                                        format = 'csv',\n",
    "                                        fields = campos,\n",
    "                                        skip_header = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'comment_text': ['you', 'are', 'a', 'very', 'nice', 'person'],\n",
       " 'toxicity': '0'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(manual_data[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator, manual_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, manual_data), \n",
    "    batch_size = 1, \n",
    "    device = device,\n",
    "    #sort_key=None,\n",
    "    sort_key=lambda x:len(x.toxicity),\n",
    "    sort_within_batch=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_test = []\n",
    "predictions_sig=[]\n",
    "labels_test=[]\n",
    "for batch in manual_iterator:\n",
    "    labels_test.append(batch.toxicity.cpu().detach().numpy())\n",
    "    predictions = mejorcito(batch.comment_text.cpu()).squeeze(1)\n",
    "    pred_sig = torch.sigmoid(predictions)\n",
    "    predictions_sig.append(pred_sig.detach().numpy())\n",
    "    rounded_preds = torch.round(torch.sigmoid(predictions))\n",
    "    prediction_test.append(rounded_preds.detach().numpy())\n",
    "    \n",
    "\n",
    "y_true = np.concatenate(labels_test)\n",
    "y_pred = np.concatenate(prediction_test)\n",
    "y_noroud = np.concatenate(predictions_sig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6, 0],\n",
       "       [3, 5]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      1.00      0.80         6\n",
      "         1.0       1.00      0.62      0.77         8\n",
      "\n",
      "    accuracy                           0.79        14\n",
      "   macro avg       0.83      0.81      0.78        14\n",
      "weighted avg       0.86      0.79      0.78        14\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_true, y_pred)\n",
    "display(cm)\n",
    "\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 0.] [0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>pred</th>\n",
       "      <th>no round</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>i love you</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.372464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>hey man you suck</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.991344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>i love you but you have to shout your mouth</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.603856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>delete your useless account</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.697962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>have a great day</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.214486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>this a fake information i do not believe it</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.225871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>valdivia is such a nice city</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.089271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>you look like a horse</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.326474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>have you no brain?!</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.442662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>you are a very nice person</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.358732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>you gave useless information get the fuck out ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.924888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>look at you fat boy</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.389761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>better save your opinion you stupid guy</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.887468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>do not spam my talk page again with regard to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004663</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         comment_text  toxicity  pred  \\\n",
       "0                                          i love you         0     0   \n",
       "1                                    hey man you suck         1     1   \n",
       "2         i love you but you have to shout your mouth         1     1   \n",
       "3                         delete your useless account         1     1   \n",
       "4                                    have a great day         0     0   \n",
       "5         this a fake information i do not believe it         0     0   \n",
       "6                        valdivia is such a nice city         0     0   \n",
       "7                               you look like a horse         1     0   \n",
       "8                                 have you no brain?!         1     0   \n",
       "9                          you are a very nice person         0     0   \n",
       "10  you gave useless information get the fuck out ...         1     1   \n",
       "11                                look at you fat boy         1     0   \n",
       "12            better save your opinion you stupid guy         1     1   \n",
       "13  do not spam my talk page again with regard to ...         0     0   \n",
       "\n",
       "    no round  \n",
       "0   0.372464  \n",
       "1   0.991344  \n",
       "2   0.603856  \n",
       "3   0.697962  \n",
       "4   0.214486  \n",
       "5   0.225871  \n",
       "6   0.089271  \n",
       "7   0.326474  \n",
       "8   0.442662  \n",
       "9   0.358732  \n",
       "10  0.924888  \n",
       "11  0.389761  \n",
       "12  0.887468  \n",
       "13  0.004663  "
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual_df = pd.read_csv('../challenge_nlp/data/manual_data.csv')\n",
    "manual_df['pred'] = [int(i) for i in y_pred]\n",
    "manual_df['no round'] = y_noroud\n",
    "manual_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
